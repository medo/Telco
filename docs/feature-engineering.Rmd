---
title: "Feature Engineering Document"
output: html_document
---
## Feature Engineering

In this document we will explain how we engineered the featured to be suitable for our model (Naive Bayes). Why Naive Bayes specifically? Please refer to the model choice document. Naive bayes is known to be vunerable to noise in the data and also doesn't take feature interactions into consideration. So we had to prepare every interaction we thought it would be useful for the model to perform well.

```{r echo=FALSE}
library(ggplot2)
library(tidyr)
library(dplyr)
library(knitr)
library(caret)
library(corrgram)
library(pROC)
library(plyr)
library(caretEnsemble)
library(doMC)
registerDoMC(cores = 4)

#roaming_data <- read.csv("data/roaming_monthly.csv")
contract_data <- read.csv("../data/contract_clean.csv")
test_data <- read.csv("../data/test.csv")
train_data <- read.csv("../data/train.csv")
daily_aggregate <- read.csv("../data/daily_aggregate.csv")
train_estimate_data <- read.csv("../data/train_estimated.csv")
train_data$ES <- train_estimate_data$ES
test_estimate_data <- read.csv("../data/test_estimated.csv")
test_data$ES <- test_estimate_data$ES
rm(train_estimate_data)
rm(test_estimate_data)

train_data$TARGET <- as.factor(train_data$TARGET)

```

### Preparing the Daily Aggregate

The daily aggregate dataset contained a row for each (user, day, cell) combination.
```{r}
daily_aggregate %>% head
daily_aggregate %>% str
```

In order to use those as features. We want to first, elemenate the cell groupping and then spread each day as a column. So that the final output contains a row per users with columns describing the consumption on each day. Some users have missing values on some days so w assumed that they didn't consume any data during that day.

```{r}
daily_aggregate_man <- daily_aggregate %>% group_by(CONTRACT_KEY, CALL_DATE_KEY) %>% summarise_each(funs(sum), TOTAL_CONSUMPTION) %>% mutate(TOTAL_CONSUMPTION=TOTAL_CONSUMPTION/1024/1024) %>% mutate(CALL_DATE_KEY=paste("USAGE_DAY_", CALL_DATE_KEY, sep="")) %>% spread(CALL_DATE_KEY, TOTAL_CONSUMPTION, fill=0)
```

### Joining Contract and Daily Aggregate dataset

Now that we have the daily aggregate and the contract dataset with a row per user. They are ready to be merged with the full training set.
```{r}
ret_data <- train_data

ret_data <- inner_join(ret_data, contract_data, by="CONTRACT_KEY") 
ret_data <- left_join(ret_data, daily_aggregate_man , by="CONTRACT_KEY") 
```

### Fixing the Age

Some ages were missing so we standerdized them all as age "99" and then replaced them with the median age.
```{r}
# FIXING THE AGE
ret_data[ret_data$AGE == 99,]$AGE <- (ret_data[ret_data$AGE != 99,]$AGE %>% median)
```

### Estimating the sixth month

The data we had contained five months and our target is releated to the sixth month. According to the exploratory analysis we did, we thought that the sixth month can be estimated by a line regression on the 5 months.

```{r eval=F}
calculate_month_estimate <- function(u1,u2,u3,u4,u5){
  d <- data.frame(
    x = (1:5),
    y =c(u1,u2,u3,u4,u5)
  )
  p <- predict(lm(y~x, data=d), data.frame(x=c(6)))[[1]]
  return(p)
}

train_es <- train_data %>% rowwise() %>% do(ES=calculate_month_estimate(.$X206_USAGE, .$X207_USAGE, .$X208_USAGE, .$X209_USAGE,.$X210_USAGE)) %>% summarise(ES=ES)
train_es <- data.frame(ES = train_es$ES %>% unlist)
write.csv(train_es, quote=FALSE,row.names=FALSE, file="./data/train_estimated.csv")

test_es <- test_data %>% rowwise() %>% do(ES=calculate_month_estimate(.$X206_USAGE, .$X207_USAGE, .$X208_USAGE, .$X209_USAGE,.$X210_USAGE)) %>% summarise(ES=ES)
test_es <- data.frame(ES = test_es$ES %>% unlist)
write.csv(test_es, quote=FALSE,row.names=FALSE, file="./data/test_estimated.csv")
```

### Adding feature interactions

As we mentioned before, Naive bayes doesn't take into consideration the feature interactions. So we had to generate the features that we thought may be usefull to the model. Most of them were inspired from the explartory analysis document.

```{r}
# Usage mean
ret_data <- ret_data %>% mutate(USAGE_MEAN=(X206_USAGE+X207_USAGE+X208_USAGE+X209_USAGE+X210_USAGE)/5)

# Difference between last and first month to detect the slope.
ret_data <- ret_data %>% mutate(USAGE_DIFF=X210_USAGE-X206_USAGE)

# Variance from the mean
ret_data <- ret_data %>% mutate(USAGE_VARIANCE=(((X206_USAGE - USAGE_MEAN)^2 + (X207_USAGE - USAGE_MEAN)^2 + (X208_USAGE - USAGE_MEAN)^2 + (X209_USAGE - USAGE_MEAN)^2 + (X210_USAGE - USAGE_MEAN)^2)/5)^0.5)
```

Plotting the estimated month againest the mean (Which are our most two important features) shows that the data can be linearly separable.

```{r}
ggplot(data=ret_data) + geom_point(aes(ES,USAGE_MEAN, color=TARGET))
```

So we thought that a boolean feature seperating those two chunks which relates those two variables is important. And it actually was one of the most important features.
```{r}
ret_data <- ret_data %>% mutate(USAGE_FARGHAL=as.factor(ES > 0.9*USAGE_MEAN + 500))
```

### Missing Daily Aggregate

For users with a missing daily aggergate of the fifth month. We replaced it with average usage per day in the fifth month.
```{r}
ret_data <- ret_data %>% mutate(USAGE_DAY_6337=ifelse(is.na(USAGE_DAY_6337), X210_USAGE/30,USAGE_DAY_6337))
ret_data <- ret_data %>% mutate(USAGE_DAY_6338=ifelse(is.na(USAGE_DAY_6338), X210_USAGE/30,USAGE_DAY_6338))
ret_data <- ret_data %>% mutate(USAGE_DAY_6339=ifelse(is.na(USAGE_DAY_6339), X210_USAGE/30,USAGE_DAY_6339))
ret_data <- ret_data %>% mutate(USAGE_DAY_6340=ifelse(is.na(USAGE_DAY_6340), X210_USAGE/30,USAGE_DAY_6340))
ret_data <- ret_data %>% mutate(USAGE_DAY_6341=ifelse(is.na(USAGE_DAY_6341), X210_USAGE/30,USAGE_DAY_6341))
ret_data <- ret_data %>% mutate(USAGE_DAY_6342=ifelse(is.na(USAGE_DAY_6342), X210_USAGE/30,USAGE_DAY_6342))
ret_data <- ret_data %>% mutate(USAGE_DAY_6343=ifelse(is.na(USAGE_DAY_6343), X210_USAGE/30,USAGE_DAY_6343))
ret_data <- ret_data %>% mutate(USAGE_DAY_6344=ifelse(is.na(USAGE_DAY_6344), X210_USAGE/30,USAGE_DAY_6344))
ret_data <- ret_data %>% mutate(USAGE_DAY_6345=ifelse(is.na(USAGE_DAY_6345), X210_USAGE/30,USAGE_DAY_6345))
ret_data <- ret_data %>% mutate(USAGE_DAY_6346=ifelse(is.na(USAGE_DAY_6346), X210_USAGE/30,USAGE_DAY_6346))
ret_data <- ret_data %>% mutate(USAGE_DAY_6347=ifelse(is.na(USAGE_DAY_6347), X210_USAGE/30,USAGE_DAY_6347))
ret_data <- ret_data %>% mutate(USAGE_DAY_6348=ifelse(is.na(USAGE_DAY_6348), X210_USAGE/30,USAGE_DAY_6348))
ret_data <- ret_data %>% mutate(USAGE_DAY_6349=ifelse(is.na(USAGE_DAY_6349), X210_USAGE/30,USAGE_DAY_6349))
ret_data <- ret_data %>% mutate(USAGE_DAY_6350=ifelse(is.na(USAGE_DAY_6350), X210_USAGE/30,USAGE_DAY_6350))
ret_data <- ret_data %>% mutate(USAGE_DAY_6351=ifelse(is.na(USAGE_DAY_6351), X210_USAGE/30,USAGE_DAY_6351))
ret_data <- ret_data %>% mutate(USAGE_DAY_6352=ifelse(is.na(USAGE_DAY_6352), X210_USAGE/30,USAGE_DAY_6352))
ret_data <- ret_data %>% mutate(USAGE_DAY_6353=ifelse(is.na(USAGE_DAY_6353), X210_USAGE/30,USAGE_DAY_6353))
ret_data <- ret_data %>% mutate(USAGE_DAY_6354=ifelse(is.na(USAGE_DAY_6354), X210_USAGE/30,USAGE_DAY_6354))
ret_data <- ret_data %>% mutate(USAGE_DAY_6355=ifelse(is.na(USAGE_DAY_6355), X210_USAGE/30,USAGE_DAY_6355))
ret_data <- ret_data %>% mutate(USAGE_DAY_6356=ifelse(is.na(USAGE_DAY_6356), X210_USAGE/30,USAGE_DAY_6356))
ret_data <- ret_data %>% mutate(USAGE_DAY_6357=ifelse(is.na(USAGE_DAY_6357), X210_USAGE/30,USAGE_DAY_6357))
ret_data <- ret_data %>% mutate(USAGE_DAY_6358=ifelse(is.na(USAGE_DAY_6358), X210_USAGE/30,USAGE_DAY_6358))
ret_data <- ret_data %>% mutate(USAGE_DAY_6359=ifelse(is.na(USAGE_DAY_6359), X210_USAGE/30,USAGE_DAY_6359))
ret_data <- ret_data %>% mutate(USAGE_DAY_6360=ifelse(is.na(USAGE_DAY_6360), X210_USAGE/30,USAGE_DAY_6360))
ret_data <- ret_data %>% mutate(USAGE_DAY_6361=ifelse(is.na(USAGE_DAY_6361), X210_USAGE/30,USAGE_DAY_6361))
ret_data <- ret_data %>% mutate(USAGE_DAY_6362=ifelse(is.na(USAGE_DAY_6362), X210_USAGE/30,USAGE_DAY_6362))
ret_data <- ret_data %>% mutate(USAGE_DAY_6363=ifelse(is.na(USAGE_DAY_6363), X210_USAGE/30,USAGE_DAY_6363))
ret_data <- ret_data %>% mutate(USAGE_DAY_6364=ifelse(is.na(USAGE_DAY_6364), X210_USAGE/30,USAGE_DAY_6364))
ret_data <- ret_data %>% mutate(USAGE_DAY_6365=ifelse(is.na(USAGE_DAY_6365), X210_USAGE/30,USAGE_DAY_6365))
```

### More features using the daily aggergate

```{r}
# A simple estimation of the first day of the sixth month
ret_data <- ret_data %>% mutate(USAGE_DAY_EXPECTED=(USAGE_DAY_6365 - USAGE_DAY_6337)/30 + USAGE_DAY_6365)

# User's usage per day
ret_data <- ret_data %>% mutate(USAGE_PER_DAY=((USAGE_DAY_6337+ USAGE_DAY_6338+ USAGE_DAY_6339+ USAGE_DAY_6340+ USAGE_DAY_6341+ USAGE_DAY_6342+ USAGE_DAY_6343+ USAGE_DAY_6344+ USAGE_DAY_6345+ USAGE_DAY_6346+ USAGE_DAY_6347+ USAGE_DAY_6348+ USAGE_DAY_6349+ USAGE_DAY_6350+ USAGE_DAY_6351+ USAGE_DAY_6352+ USAGE_DAY_6353+ USAGE_DAY_6354+ USAGE_DAY_6355+ USAGE_DAY_6356+ USAGE_DAY_6357+ USAGE_DAY_6358+ USAGE_DAY_6359+ USAGE_DAY_6360+ USAGE_DAY_6361+ USAGE_DAY_6362+ USAGE_DAY_6363+ USAGE_DAY_6364+ USAGE_DAY_6365)/30))

# Difference between last and first day to show the trend
ret_data <- ret_data %>% mutate(USAGE_DAY_DIFF=USAGE_DAY_6365 - USAGE_DAY_6337)
```

### Grouping Rate Plan

With a simple data exploration, we can group users with a rate plan containing business or enterprise.
```{r}
ret_data$RATE_PLAN <- ret_data$RATE_PLAN %>% as.character %>% sapply(function(x) ifelse(grepl("enterprise|business", tolower(x)), "Business", x)) %>% as.factor
```

### Trend Boolean

Plotting the usage diff shows a nice feature :
```{r}
ggplot(data=ret_data) + geom_point(aes(1:nrow(ret_data), USAGE_DIFF, color=TARGET))
```

As shown from the plot. The data can be linearly separable at DIFF=0. Those who have difference greater than zero (Positive slope) tend to have a target 1 and vice versa.
```{r}
ret_data <- ret_data %>% mutate(USAGE_DIFF_POS=as.factor(USAGE_DIFF > 0))
```

### Removing noise

As we mentioned before. Noise affects Naive Bayes badly. Removing the outliers increased the model's performance.

```{r}
ret_data <- ret_data %>% filter(USAGE_MEAN < quantile(ret_data$USAGE_MEAN, c(0.99))[[1]])

es_99_quantile <- quantile(ret_data$ES, c(0.99))[[1]]
es_1_quantile <- quantile(ret_data$ES, c(0.01))[[1]]
ret_data <- ret_data %>% filter(ES < es_99_quantile)
ret_data <- ret_data %>% filter(ES > es_1_quantile)

ret_data <- ret_data %>% filter(USAGE_PER_DAY < quantile(ret_data$USAGE_PER_DAY, c(0.99))[[1]])
ret_data <- ret_data %>% filter(USAGE_DAY_EXPECTED < quantile(ret_data$USAGE_DAY_EXPECTED, c(0.99))[[1]])
ret_data <- ret_data %>% filter(USAGE_DIFF > quantile(ret_data$USAGE_DIFF, c(0.01))[[1]])
ret_data <- ret_data %>% filter(USAGE_DAY_6365 < quantile(ret_data$USAGE_DAY_6365, c(0.99))[[1]])

levels(ret_data$TARGET) <- make.names(levels(factor(ret_data$TARGET)))
```
  
### Dropping Correlated columns

Most of the dropped columns here was based on trial and error naive bayes.

```{r}
# Drop UNNEEDED COLUMNS
#ret_data <- ret_data %>% dplyr::select(-ends_with("_SESSION_COUNT"))
#ret_data <- ret_data %>% dplyr::select(-starts_with("SESSIONS_"))
ret_data <- ret_data %>% dplyr::select(-matches("X20"))
ret_data <- ret_data %>% dplyr::select(-num_range("USAGE_DAY_",6337:6364))
ret_data <- ret_data %>% dplyr::select(-USAGE_PER_DAY)

# Removing almost zero variance variables
#nzv <- nearZeroVar(ret_data, names=TRUE)
#nzv <- nzv [! nzv %in% c("TARGET")]
#ret_data <- ret_data %>% dplyr::select(-one_of(nzv))

# Removing highly correlated features
#descrCor <- cor(ret_data %>% dplyr::select(-TARGET))
#highlyCorDescr <- findCorrelation(descrCor, cutoff = .90, names=TRUE)
#ret_data <- ret_data %>% dplyr::select(-one_of(highlyCorDescr))

# Removing linearly corelated values
#lc <- findLinearCombos(ret_data %>% dplyr::select(-TARGET))

#if(!is.null(lc$remove)){
#  ret_data <- ret_data[, -lc$remove]
#}
```