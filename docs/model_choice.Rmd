---
title: "Model Choice Document"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Models

We used different models against the dataset features. However, Naive Bayes (Gaussian) outperformed them all.
The models we used are:

- LogisticRegression
- DecesionTree
- RandomForest
- KNN
- SVM (radial and linear).
- Stocastic Gradient Descent
- Neural Network (different structures upto 7 layers)
- GBM
- AdaBoost
- XGBtree
- Ensembling some of these classifers
- NaiveBayes

In most of the cases naive bayes outperformed all other models. Thus, we decided to go for NaiveBayes and designed our feature engineering to work well with naive bayes.

## SVM

The linear SVM did very poorly on the training data. And the radial SVM took more than 7 hours to train and didn't finish so we decided to ignore it.

## KNN

We did a grid search on KNN's params (e.g. K) but they all didn't work as good as the naive bayes.

## Neural Network

We implemented a 7 layer neural network with 100 neuron in each layer (experimented other structures) using Keras on top of Theano which runs over the GPU (refer to `deeplearning.py`) We used 500 epochs and 100 as the batch size.The neural network scored Fscore = 0.59 after scaling. 

## Ensembling

We had three approches for ensembling. The first one was using weighted voting. We ensembled 4 submissions (NaiveBayes, RandomForest, KNN, DecisionTree). NB had the heighest weight as it was the best submission. It scored an f-score of 0.64. The second trial was ORing each prediction as the TARGET=1 was a bit rare. It scored 0.67 which was the same as the NaiveBayes submission. We called it "FarghalClassifier" and it's in `submit.ipynb`. The third trial was using the caret library `caretEnsemble`. We ensembled different models which weren't highly correlated, but the resulted model scored lower than our best models.

Finally, naive bayes started outperforming all the models and it doesn't have many params to tune so there wasn't much room for ensembling.

## Naive Bayes

Naive Bayes is one of the simple and powerful machine learning models that is less likely to overfit.

### Probabilistic Model

$posterior = \dfrac{prior \times likelihood }{evidence}$

The important factors here are the likelihood of the class to occuer in the trained data and the prior which is an assumption about the probability distribution of this class to occuer. we used a Gaussian assumption which worked well with the data. 

### Features

One important assumption in naive bayes that the variables (features) are independent (conditional independence), this makes naive bayes able to calcualte the likelihood easily. The problem with this is that it cannot capture the feature interaction which is important in our case. For example, we want to model to caputre that weather the predicted usage is greater than the mean or not. Thus, we engineered some feature that does this for example: USAGE_DIFF, USAGE_FARGHAL, Usage_DIFF_POS. All these feature try to caputre the feature interactions. Moreover, although naive bayes assumes the conditional independence but practically it still performs very well even if the variables had some dependencies between them.

### Categorical

On most models we used we had to encode the cateogrical using One-Hot Encoding. However, in naive bayes we did not do this because it works well with cateogrical features, we even found out that it works with categorical features better than numerical features thus we converted the boolean values to categorical because this make the model calculate 'count' the occurences of the categorical value to calculate the likelihood.
