---
title: "Model Choice Document"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Models

We used different models against the dataset features. However, Naive Bayes (Gaussian) outperformed them all.
The models we used are:

- LogisticRegression
- DecesionTree
- RandomForest
- KNN
- SVM (radial and linear) but it took hours thus we did not evaluate it.
- Stocastic Gradient Descent
- Neural Network (different structures upto 7 layers)
- GBM
- AdaBoost
- XGBtree
- Ensumbling some of these classifers
- NaiveBayes

In most of the cases naive bayes outperformed all other models. Thus, we decided to go for NaiveBayes and designed our feature engineering to work well with naive bayes.

## Ensembling

## Neural Network

We implemented a 7 layer neural network with 100 neuron in each layer (experimented other structures) using Keras on top of Theano which runs over the GPU (refer to deeplearning.py) We used 500 epochs and 100 as the batch size.The neural network scored Fscore = 0.59 after scaling. 

## Naive Bayes

Naive Bayes is one of the simple and powerful machine learning models that is less likely to overfit.

### Probabilistic Model

$posterior = \dfrac{prior \times likelihood }{evidence}$

The important factors here are the likelihood of the class to occuer in the trained data and the prior which is an assumption about the probability distribution of this class to occuer. we used a Gaussian assumption which worked well with the data. 

### Features

One important assumption in naive bayes that the variables (features) are independent (conditional independence), this makes naive bayes able to calcualte the likelihood easily. The problem with this is that it cannot capture the feature interaction which is important in our case. For example, we want to model to caputre that weather the predicted usage is greater than the mean or not. Thus, we engineered some feature that does this for example: USAGE_DIFF, USAGE_FARGHAL, Usage_DIFF_POS. All these feature try to caputre the feature interactions. Moreover, although naive bayes assumes the conditional independence but practically it still performs very well even if the variables had some dependencies between them.

### Categorical

On most models we used we had to encode the cateogrical using One-Hot Encoding. However, in naive bayes we did not do this because it works well with cateogrical features, we even found out that it works with categorical features better than numerical features thus we converted the boolean values to categorical because this make the model calculate 'count' the occurences of the categorical value to calculate the likelihood.
